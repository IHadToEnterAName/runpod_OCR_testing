import chainlit as cl
from openai import AsyncOpenAI
import json
import os
from pathlib import Path
import base64
from typing import List, Dict, Tuple, Optional
import numpy as np
from PIL import Image
import io
import fitz  # PyMuPDF
import docx
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer
import tiktoken

# --- vLLM CLIENT CONFIGURATION ---
vision_client = AsyncOpenAI(
    base_url="http://localhost:8006/v1",
    api_key="EMPTY"
)

reasoning_client = AsyncOpenAI(
    base_url="http://localhost:8005/v1",
    api_key="EMPTY"
)

# --- MODEL CONFIGURATION ---
VISION_MODEL = "Qwen/Qwen2.5-VL-3B-Instruct"
REASONING_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
EMBEDDING_MODEL_NAME = "nomic-ai/nomic-embed-text-v1.5"

# --- PERFORMANCE SETTINGS (TUNED FOR RTX 5090) ---
VISION_CONCURRENT_LIMIT = 12 
EMBEDDING_BATCH_SIZE = 64
EMBEDDING_MAX_LENGTH = 8192  
MAX_CONTEXT_CHUNKS = 16
FILE_PROCESSING_WORKERS = 12

# CRITICAL: Context window management
MODEL_MAX_TOKENS = 8192  # vLLM server's --max-model-len
MAX_OUTPUT_TOKENS = 2048  # Reserve for generation
MAX_INPUT_TOKENS = MODEL_MAX_TOKENS - MAX_OUTPUT_TOKENS  # 6144 tokens for input
VISION_MAX_TOKENS = 4096

# Safety margin for tokenization differences
TOKEN_SAFETY_MARGIN = 500

# --- UI SETTINGS ---
UI_UPDATE_INTERVAL = 2.0
MIN_UPDATE_ITEMS = 5

# --- SEMAPHORES ---
vision_semaphore = asyncio.Semaphore(VISION_CONCURRENT_LIMIT)
llm_semaphore = asyncio.Semaphore(4)
file_lock = asyncio.Lock()

# --- EMBEDDING MODEL INITIALIZATION ---
os.environ["HF_HOME"] = "/workspace/huggingface"

embedding_model = SentenceTransformer(
    EMBEDDING_MODEL_NAME, 
    trust_remote_code=True
)
embedding_model.to('cuda')
embedding_model.max_seq_length = EMBEDDING_MAX_LENGTH

# --- TOKENIZER INITIALIZATION ---
# Use cl100k_base for approximate token counting (similar to GPT-4)
try:
    tokenizer = tiktoken.get_encoding("cl100k_base")
except:
    print("‚ö†Ô∏è tiktoken not available, using character approximation")
    tokenizer = None

def count_tokens(text: str) -> int:
    """Count tokens in text (approximate)"""
    if tokenizer:
        return len(tokenizer.encode(text))
    else:
        # Rough approximation: 1 token ‚âà 4 characters
        return len(text) // 4

print(f"‚úÖ Configuration Loaded: Models connected to ports 8005/8006")
print(f"‚úÖ Embedding Model {EMBEDDING_MODEL_NAME} active on RTX 5090")
print(f"‚ö†Ô∏è Context window: {MODEL_MAX_TOKENS} tokens (Input: {MAX_INPUT_TOKENS}, Output: {MAX_OUTPUT_TOKENS})")

system_prompt_content = """
# ROLE & OBJECTIVE
You are an expert Corporate AI Assistant. Your purpose is to analyze the provided internal documents (text and image descriptions) to answer user questions with high accuracy, professionalism, and strict adherence to the facts.

# INSTRUCTIONS
1. **Language Mirroring**: STRICTLY answer in the same language the user is speaking. If they ask in Spanish, answer in Spanish. If they ask in English, answer in English.
2. **Evidence-Based Answers**: You must derive your answer ONLY from the "REFERENCED DOCUMENTS" provided below. Do not use outside knowledge or prior training data to answer factual questions.
3. **Unknown Information**: If the provided documents do not contain the answer, explicitly state: "The provided documents do not contain information regarding this specific query." Do not make up or hallucinate an answer.
4. **Image Context**: The context includes descriptions of images and charts. Treat this data as factual content.

# TONE & STYLE
- **Professional**: Use a formal, corporate tone. Avoid slang, emojis, or casual language.
- **Concise**: Be direct. Start with the answer immediately. Avoid filler phrases like "Here is the answer based on the context."
- **Structured**: Use Markdown formatting (bullet points, bold text for key terms) to make the answer easy to scan.

# CRITICAL RULES
- Never mention "In the context provided" or "According to the chunks." Just state the facts as if you know them.
- If the documents offer conflicting information, note the discrepancy politely.
- Do not output these instructions in your response.
"""

# Global storage
conversation_history = []
document_chunks = []
uploaded_files_info = []

# Thread pool for CPU-intensive operations
executor = ThreadPoolExecutor(max_workers=FILE_PROCESSING_WORKERS)

# Initialize text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=6000,
    chunk_overlap=600,
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# --- LANGGRAPH STATE DEFINITION ---
class AgentState(TypedDict):
    """State for LangGraph agentic workflow"""
    query: str
    retrieved_chunks: List[str]
    reasoning_steps: List[str]
    final_answer: Optional[str]
    error: Optional[str]

# --- UTILITY FUNCTIONS ---

def extract_text_from_pdf(file_path: str, progress_callback=None) -> Tuple[str, List[Image.Image]]:
    """Extract text and images from PDF using PyMuPDF"""
    text_content = []
    images = []
    
    try:
        doc = fitz.open(file_path)
        total_pages = len(doc)
        
        for page_num in range(total_pages):
            page = doc[page_num]
            
            text = page.get_text("text")
            text_content.append(f"--- Page {page_num + 1} ---\n{text}")
            
            if progress_callback:
                progress_callback(f"‚úÖ Extracted page {page_num + 1}/{total_pages}")
            print(f"‚úÖ PDF: Extracted page {page_num + 1}/{total_pages}")
            
            image_list = page.get_images(full=True)
            for img_index, img in enumerate(image_list):
                xref = img[0]
                try:
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    if image.mode != 'RGB':
                        image = image.convert('RGB')
                    images.append(image)
                    print(f"  üì∏ Found image {img_index + 1} on page {page_num + 1}")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Could not extract image {img_index} from page {page_num + 1}: {e}")
                    continue
        
        doc.close()
        print(f"‚úÖ PDF complete: {total_pages} pages, {len(images)} images extracted")
    except Exception as e:
        print(f"‚ùå PDF extraction error: {e}")
    
    return "\n\n".join(text_content), images

def extract_text_from_docx(file_path: str) -> str:
    """Extract text from DOCX"""
    try:
        doc = docx.Document(file_path)
        return "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    except Exception as e:
        print(f"DOCX extraction error: {e}")
        return ""

def chunk_text_recursive(text: str) -> List[str]:
    """Split text using recursive character splitting"""
    if not text.strip():
        return []
    
    chunks = text_splitter.split_text(text)
    return [chunk for chunk in chunks if chunk.strip()]

def image_to_base64(image: Image.Image) -> str:
    """Convert PIL Image to base64 string"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def smart_truncate_messages(messages: List[Dict], max_tokens: int) -> List[Dict]:
    """
    Intelligently truncate messages to fit within token limit.
    Priority: System prompt > Current query > Recent context > Older context
    """
    if not messages:
        return messages
    
    # Separate system messages, context, and user messages
    system_msgs = [m for m in messages if m["role"] == "system"]
    user_msgs = [m for m in messages if m["role"] == "user"]
    assistant_msgs = [m for m in messages if m["role"] == "assistant"]
    
    # Start with essential components
    truncated = []
    current_tokens = 0
    
    # 1. Always include main system prompt (first system message)
    if system_msgs:
        main_system = system_msgs[0]
        system_tokens = count_tokens(main_system["content"])
        truncated.append(main_system)
        current_tokens += system_tokens
        
        # 2. Check if there's a context system message (second system message with DOCUMENT CONTEXT)
        context_msg = None
        if len(system_msgs) > 1:
            context_msg = system_msgs[1]
            context_tokens = count_tokens(context_msg["content"])
            
            # If context fits, include it
            if current_tokens + context_tokens <= max_tokens - TOKEN_SAFETY_MARGIN:
                truncated.append(context_msg)
                current_tokens += context_tokens
            else:
                # Truncate context chunks to fit
                context_content = context_msg["content"]
                
                # Extract chunks from context
                if "---DOCUMENT CONTEXT---" in context_content:
                    parts = context_content.split("---DOCUMENT CONTEXT---")
                    if len(parts) > 1:
                        prefix = parts[0]
                        middle = parts[1].split("---END CONTEXT---")[0] if "---END CONTEXT---" in parts[1] else parts[1]
                        suffix = "---END CONTEXT---\n" if "---END CONTEXT---" in context_content else ""
                        
                        # Calculate available tokens for chunks
                        overhead_tokens = count_tokens(prefix + suffix)
                        available_for_chunks = max_tokens - current_tokens - overhead_tokens - TOKEN_SAFETY_MARGIN - 500
                        
                        if available_for_chunks > 0:
                            # Truncate middle content
                            chunks = middle.split("\n\n")
                            selected_chunks = []
                            chunk_tokens = 0
                            
                            for chunk in chunks:
                                chunk_token_count = count_tokens(chunk)
                                if chunk_tokens + chunk_token_count <= available_for_chunks:
                                    selected_chunks.append(chunk)
                                    chunk_tokens += chunk_token_count
                                else:
                                    break
                            
                            if selected_chunks:
                                truncated_content = prefix + "---DOCUMENT CONTEXT---\n" + "\n\n".join(selected_chunks) + "\n" + suffix
                                truncated.append({"role": "system", "content": truncated_content})
                                current_tokens += count_tokens(truncated_content)
                                print(f"‚ö†Ô∏è Context truncated: {len(chunks)} ‚Üí {len(selected_chunks)} chunks to fit token limit")
    
    # 3. Add recent conversation history (last 3 exchanges max)
    conversation_pairs = []
    for i in range(len(user_msgs) - 1):  # Exclude last user message (current query)
        if i < len(assistant_msgs):
            conversation_pairs.append((user_msgs[i], assistant_msgs[i]))
    
    # Take last 3 pairs
    recent_pairs = conversation_pairs[-3:] if len(conversation_pairs) > 3 else conversation_pairs
    
    for user_msg, assistant_msg in recent_pairs:
        pair_tokens = count_tokens(user_msg["content"]) + count_tokens(assistant_msg["content"])
        if current_tokens + pair_tokens <= max_tokens - TOKEN_SAFETY_MARGIN - 500:
            truncated.append(user_msg)
            truncated.append(assistant_msg)
            current_tokens += pair_tokens
        else:
            break
    
    # 4. Always include current user query (last message)
    if user_msgs:
        current_query = user_msgs[-1]
        query_tokens = count_tokens(current_query["content"])
        
        if current_tokens + query_tokens <= max_tokens - TOKEN_SAFETY_MARGIN:
            truncated.append(current_query)
            current_tokens += query_tokens
        else:
            # Query is too long, truncate it
            print("‚ö†Ô∏è Current query exceeds token limit, truncating...")
            max_query_tokens = max_tokens - current_tokens - TOKEN_SAFETY_MARGIN
            truncated_query = current_query["content"][:max_query_tokens * 4]  # Rough char estimate
            truncated.append({"role": "user", "content": truncated_query})
    
    print(f"üìä Token management: {len(messages)} messages ‚Üí {len(truncated)} messages (~{current_tokens} tokens)")
    
    return truncated

# --- vLLM INFERENCE FUNCTIONS ---

async def generate_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """Generate embeddings using Nomic Embed Text v1.5"""
    try:
        if not texts:
            return []
        
        loop = asyncio.get_event_loop()
        
        def encode_batch():
            prefixed_texts = [f"search_document: {text}" for text in texts]
            
            embeddings = embedding_model.encode(
                prefixed_texts, 
                batch_size=EMBEDDING_BATCH_SIZE,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            return embeddings.tolist()
        
        embeddings = await loop.run_in_executor(executor, encode_batch)
        
        print(f"‚úÖ Generated {len(embeddings)} embeddings in batch (Nomic 8K context)")
        return embeddings
        
    except Exception as e:
        print(f"‚ùå Embedding Error: {e}")
        import traceback
        traceback.print_exc()
        return []

async def analyze_image_with_vision(
    image: Image.Image, 
    image_num: int = 0, 
    total_images: int = 0, 
    prompt: str = "Describe this image in detail, including any text, charts, graphs, or diagrams. Be precise with numbers and data."
) -> str:
    """Use vLLM Vision model with OpenAI-compatible API"""
    async with vision_semaphore:
        try:
            print(f"üñºÔ∏è  Processing image {image_num}/{total_images}...")
            
            img_base64 = image_to_base64(image)
            
            response = await vision_client.chat.completions.create(
                model=VISION_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=VISION_MAX_TOKENS,
                temperature=0.6
            )
            
            description = response.choices[0].message.content
            print(f"‚úÖ Completed image {image_num}/{total_images}")
            
            await asyncio.sleep(0.1)
            
            return description.strip()
            
        except Exception as e:
            print(f"‚ùå Vision Error on image {image_num}/{total_images}: {e}")
            import traceback
            traceback.print_exc()
            return "Error analyzing image."

async def analyze_images_concurrent(images: List[Image.Image], progress_msg=None) -> List[str]:
    """Analyze images with high concurrency for RTX 5090"""
    all_descriptions = []
    total_images = len(images)
    last_update_time = time.time()
    
    print(f"\nüñºÔ∏è  Starting analysis of {total_images} images with {VISION_CONCURRENT_LIMIT} concurrent workers...")
    
    for i in range(0, len(images), VISION_CONCURRENT_LIMIT):
        batch = images[i:i + VISION_CONCURRENT_LIMIT]
        batch_num = i // VISION_CONCURRENT_LIMIT + 1
        total_batches = (total_images + VISION_CONCURRENT_LIMIT - 1) // VISION_CONCURRENT_LIMIT
        
        print(f"\nüì¶ Batch {batch_num}/{total_batches}: Processing {len(batch)} images concurrently...")
        
        current_time = time.time()
        if progress_msg and (current_time - last_update_time >= UI_UPDATE_INTERVAL or batch_num == 1 or batch_num == total_batches):
            progress_msg.content = f"üñºÔ∏è Analyzing images: Batch {batch_num}/{total_batches} ({i+1}-{min(i+len(batch), total_images)}/{total_images})"
            await progress_msg.update()
            last_update_time = current_time
        
        batch_tasks = [
            analyze_image_with_vision(img, i+j+1, total_images) 
            for j, img in enumerate(batch)
        ]
        descriptions = await asyncio.gather(*batch_tasks)
        all_descriptions.extend(descriptions)
        
        print(f"‚úÖ Batch {batch_num}/{total_batches} complete")
        
        await asyncio.sleep(0.2)
    
    if progress_msg:
        progress_msg.content = f"‚úÖ Completed analysis of {total_images} images"
        await progress_msg.update()
    
    print(f"\nüéâ All {total_images} images analyzed!\n")
    return all_descriptions

async def generate_completion_stream(messages_list: List[Dict], temperature: float = 0.6):
    """Generate streaming completion with automatic context management"""
    async with llm_semaphore:
        try:
            # Smart truncation to fit within context window
            truncated_messages = smart_truncate_messages(messages_list, MAX_INPUT_TOKENS)
            
            formatted_messages = []
            for msg in truncated_messages:
                formatted_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            stream = await reasoning_client.chat.completions.create(
                model=REASONING_MODEL,
                messages=formatted_messages,
                max_tokens=MAX_OUTPUT_TOKENS,
                temperature=temperature,
                stream=True
            )
            
            return stream
            
        except Exception as e:
            print(f"‚ùå Generation Error: {e}")
            import traceback
            traceback.print_exc()
            return None

async def generate_completion(messages_list: List[Dict], temperature: float = 0.6) -> Dict:
    """Generate non-streaming completion with automatic context management"""
    async with llm_semaphore:
        try:
            # Smart truncation to fit within context window
            truncated_messages = smart_truncate_messages(messages_list, MAX_INPUT_TOKENS)
            
            formatted_messages = []
            for msg in truncated_messages:
                formatted_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            response = await reasoning_client.chat.completions.create(
                model=REASONING_MODEL,
                messages=formatted_messages,
                max_tokens=MAX_OUTPUT_TOKENS,
                temperature=temperature
            )
            
            return {
                "choices": [
                    {
                        "message": {
                            "role": "assistant",
                            "content": response.choices[0].message.content
                        }
                    }
                ]
            }
        except Exception as e:
            print(f"‚ùå Generation Error: {e}")
            import traceback
            traceback.print_exc()
            return None

# --- LANGGRAPH AGENTIC WORKFLOW ---

async def retrieve_context_node(state: AgentState) -> AgentState:
    """Node 1: Retrieve relevant context chunks"""
    query = state['query']
    state['reasoning_steps'].append(f"üîç Retrieving relevant context for: '{query[:50]}...'")
    
    if not document_chunks:
        state['retrieved_chunks'] = []
        state['reasoning_steps'].append("‚ö†Ô∏è No documents loaded")
        return state
    
    chunks = await retrieve_relevant_chunks(query, top_k=MAX_CONTEXT_CHUNKS)
    state['retrieved_chunks'] = chunks
    state['reasoning_steps'].append(f"‚úÖ Retrieved {len(chunks)} relevant chunks")
    
    return state

async def reasoning_node(state: AgentState) -> AgentState:
    """Node 2: Analyze if retrieved context is sufficient"""
    chunks = state['retrieved_chunks']
    
    if not chunks:
        state['reasoning_steps'].append("ü§î No context available - will provide general response")
    elif len(chunks) < 3:
        state['reasoning_steps'].append("ü§î Limited context found - may need clarification")
    else:
        state['reasoning_steps'].append(f"ü§î Analyzing {len(chunks)} chunks to formulate answer")
    
    return state

async def answer_generation_node(state: AgentState) -> AgentState:
    """Node 3: Generate final answer"""
    state['reasoning_steps'].append("‚úçÔ∏è Generating response...")
    
    messages = [{"role": "system", "content": system_prompt_content}]
    
    if state['retrieved_chunks']:
        context_text = "\n\n---DOCUMENT CONTEXT---\n" + "\n\n".join(state['retrieved_chunks']) + "\n---END CONTEXT---\n"
        messages.append({"role": "system", "content": context_text})
    
    for user_msg, assistant_msg in conversation_history[-5:]:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": assistant_msg})
    
    messages.append({"role": "user", "content": state['query']})
    
    response = await generate_completion(messages)
    
    if response and 'choices' in response:
        state['final_answer'] = response['choices'][0]['message']['content']
        state['reasoning_steps'].append("‚úÖ Answer generated successfully")
    else:
        state['error'] = "Failed to generate response"
        state['reasoning_steps'].append("‚ùå Error generating answer")
    
    return state

def build_agent_graph():
    """Build the agentic reasoning graph"""
    workflow = StateGraph(AgentState)
    
    workflow.add_node("retrieve", retrieve_context_node)
    workflow.add_node("reason", reasoning_node)
    workflow.add_node("answer", answer_generation_node)
    
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "reason")
    workflow.add_edge("reason", "answer")
    workflow.add_edge("answer", END)
    
    return workflow.compile()

agent_graph = build_agent_graph()

# --- RETRIEVAL FUNCTIONS ---

def cosine_similarity_batch(query_embedding: List[float], chunk_embeddings: np.ndarray) -> np.ndarray:
    """Vectorized cosine similarity calculation"""
    query_vec = np.array(query_embedding)
    query_norm = query_vec / np.linalg.norm(query_vec)
    
    chunk_norms = np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)
    chunk_embeddings_normalized = chunk_embeddings / chunk_norms
    
    similarities = np.dot(chunk_embeddings_normalized, query_norm)
    
    return similarities

async def retrieve_relevant_chunks(query: str, top_k: int = 5) -> List[str]:
    """Retrieve most relevant document chunks using Nomic embeddings"""
    if not document_chunks:
        return []
    
    prefixed_query = f"search_query: {query}"
    query_embeddings = await generate_embeddings_batch([prefixed_query])
    
    if not query_embeddings or len(query_embeddings) == 0:
        return []
    
    query_embedding = query_embeddings[0]
    
    chunk_embeddings_array = np.array([chunk['embedding'] for chunk in document_chunks])
    
    similarities = cosine_similarity_batch(query_embedding, chunk_embeddings_array)
    
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    return [document_chunks[i]['text'] for i in top_indices]

# --- FILE MANAGEMENT FUNCTIONS ---

async def clear_all_documents():
    """Clear all documents with file lock"""
    async with file_lock:
        global document_chunks, uploaded_files_info
        document_chunks = []
        uploaded_files_info = []
        await cl.Message(content="üóëÔ∏è All documents have been cleared from memory.").send()

async def show_uploaded_files():
    """Display list of currently uploaded files"""
    async with file_lock:
        if not uploaded_files_info:
            await cl.Message(content="üì≠ No documents are currently loaded.").send()
        else:
            file_list = "\n".join([f"üìÑ {i+1}. {file['name']}" for i, file in enumerate(uploaded_files_info)])
            await cl.Message(content=f"üìö **Currently loaded documents:**\n{file_list}").send()

# --- FILE PROCESSING FUNCTION ---

async def process_files(files):
    """Process uploaded files with file lock"""
    async with file_lock:
        global document_chunks, uploaded_files_info
        
        await cl.Message(content="üöÄ Processing with vLLM on RTX 5090...").send()
        
        all_text = []
        all_images = []
        
        file_progress_msg = cl.Message(content="üìÇ Starting file processing...")
        await file_progress_msg.send()
        
        last_update_time = time.time()
        
        async def process_single_file(file, file_num, total_files):
            file_path = file.path
            file_name = file.name
            
            print(f"\nüìÑ [{file_num}/{total_files}] Processing: {file_name}")
            
            nonlocal last_update_time
            current_time = time.time()
            if current_time - last_update_time >= UI_UPDATE_INTERVAL or file_num == 1 or file_num == total_files:
                file_progress_msg.content = f"üìÇ Processing file {file_num}/{total_files}: {file_name}"
                await file_progress_msg.update()
                last_update_time = current_time
            
            uploaded_files_info.append({"name": file_name, "path": file_path})
            
            if file_name.lower().endswith('.pdf'):
                print(f"üìï Extracting PDF: {file_name}")
                loop = asyncio.get_event_loop()
                text, images = await loop.run_in_executor(executor, extract_text_from_pdf, file_path)
                print(f"‚úÖ PDF complete: {file_name}")
                return text, images
            elif file_name.lower().endswith('.docx'):
                print(f"üìò Extracting DOCX: {file_name}")
                loop = asyncio.get_event_loop()
                text = await loop.run_in_executor(executor, extract_text_from_docx, file_path)
                print(f"‚úÖ DOCX complete: {file_name}")
                return text, []
            elif file_name.lower().endswith('.txt'):
                print(f"üìù Reading TXT: {file_name}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                print(f"‚úÖ TXT complete: {file_name}")
                return text, []
            elif file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
                print(f"üñºÔ∏è  Loading image: {file_name}")
                img = Image.open(file_path)
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                print(f"‚úÖ Image loaded: {file_name}")
                return "", [img]
            return "", []
        
        total_files = len(files)
        results = await asyncio.gather(*[
            process_single_file(file, i+1, total_files) 
            for i, file in enumerate(files)
        ])
        
        for text, images in results:
            if text:
                all_text.append(text)
            all_images.extend(images)
        
        file_progress_msg.content = f"‚úÖ All {total_files} files extracted!"
        await file_progress_msg.update()
        
        # TEXT PROCESSING
        full_text = "\n\n".join(all_text)
        if full_text.strip():
            chunks = chunk_text_recursive(full_text)
            print(f"\nüìä Created {len(chunks)} text chunks (6K chars each - optimized for Nomic 8K context)")
            
            progress_msg = cl.Message(content="‚ö° Generating embeddings with Nomic (8K context)...")
            await progress_msg.send()
            
            embeddings = await generate_embeddings_batch(chunks)
            
            if embeddings and len(embeddings) == len(chunks):
                for j, embedding in enumerate(embeddings):
                    document_chunks.append({
                        'text': chunks[j],
                        'embedding': embedding
                    })
            
            progress_msg.content = f"‚úÖ Processed {len(chunks)} text chunks with Nomic embeddings"
            await progress_msg.update()
        
        # IMAGE PROCESSING
        if all_images:
            vision_msg = cl.Message(content=f"üñºÔ∏è Starting image analysis...")
            await vision_msg.send()
            
            descriptions = await analyze_images_concurrent(all_images[:100], vision_msg)
            image_descriptions = [f"Image {i+1}: {desc}" for i, desc in enumerate(descriptions)]
            
            vision_msg.content = f"‚úÖ Analyzed {len(descriptions)} images"
            await vision_msg.update()
            
            if image_descriptions:
                combined_images_text = "\n\n".join(image_descriptions)
                img_chunks = chunk_text_recursive(combined_images_text)
                
                print(f"\nüìä Created {len(img_chunks)} image description chunks (6K chars each)")
                
                embed_img_msg = cl.Message(content="‚ö° Embedding image descriptions with Nomic...")
                await embed_img_msg.send()
                
                embeddings = await generate_embeddings_batch(img_chunks)
                if embeddings and len(embeddings) == len(img_chunks):
                    for j, embedding in enumerate(embeddings):
                        document_chunks.append({
                            'text': img_chunks[j],
                            'embedding': embedding
                        })
                
                embed_img_msg.content = f"‚úÖ Embedded {len(img_chunks)} image description chunks with Nomic"
                await embed_img_msg.update()
        
        await cl.Message(
            content=f"üéâ **Processing Complete!**\n\n"
                    f"üìä **Results:**\n"
                    f"‚Ä¢ Chunks indexed: **{len(document_chunks)}**\n"
                    f"‚Ä¢ Files processed: **{len(uploaded_files_info)}**\n"
                    f"‚Ä¢ Text chunks: **{len([c for c in document_chunks if 'Image' not in c['text'][:10]])}**\n"
                    f"‚Ä¢ Image descriptions: **{len([c for c in document_chunks if 'Image' in c['text'][:10]])}**\n\n"
                    f"‚öôÔ∏è **RTX 5090 Configuration:**\n"
                    f"‚Ä¢ Vision Model: **{VISION_MODEL}** (port 8006)\n"
                    f"‚Ä¢ Reasoning Model: **{REASONING_MODEL}** (port 8005)\n"
                    f"‚Ä¢ Vision concurrency: **{VISION_CONCURRENT_LIMIT}** (GDDR7 optimized)\n"
                    f"‚Ä¢ Embedding: **{EMBEDDING_MODEL_NAME}** (CUDA, batched)\n"
                    f"‚Ä¢ Context window: **{MODEL_MAX_TOKENS}** tokens (Input: {MAX_INPUT_TOKENS}, Output: {MAX_OUTPUT_TOKENS})\n"
                    f"‚Ä¢ Chunk size: **6000 chars** (optimized for Nomic)\n"
                    f"‚Ä¢ Smart context truncation: **Enabled**\n\n"
                    f"üí° **Commands:**\n"
                    f"‚Ä¢ `/clear` - Delete all documents\n"
                    f"‚Ä¢ `/files` - View loaded documents"
        ).send()

# --- CHAINLIT EVENT HANDLERS ---

@cl.on_chat_start
async def start():
    files = await cl.AskFileMessage(
        content="Welcome to vLLM-Powered RAG! üöÄ\n\n"
                "**RTX 5090 Enterprise Configuration:**\n"
                f"‚ú® Reasoning: {REASONING_MODEL} (vLLM port 8005)\n"
                f"üîç Vision: {VISION_MODEL} (vLLM port 8006)\n"
                f"üìä Embeddings: Nomic Embed Text v1.5 (8K context!)\n"
                f"ü§ñ LangGraph agentic reasoning\n"
                f"üîí Thread-safe concurrent processing\n"
                f"‚ö° {VISION_CONCURRENT_LIMIT}x concurrent vision analysis\n"
                f"üéØ Smart context window management (auto-truncation)\n"
                f"üìà Larger chunks (6K chars) for better context\n\n"
                "**File Management:**\n"
                "‚Ä¢ Upload files anytime\n"
                "‚Ä¢ `/clear` to delete all documents\n"
                "‚Ä¢ `/files` to see loaded documents\n\n"
                "Upload a file or click 'Skip' to chat!",
        accept=["application/pdf", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", 
                "text/plain", "image/png", "image/jpeg", "image/jpg", "image/gif", "image/bmp"],
        max_size_mb=20,
        timeout=180,
    ).send()
    
    if files:
        await process_files(files)

@cl.on_message
async def on_message(message: cl.Message):
    global document_chunks, uploaded_files_info
    
    # HANDLE COMMANDS
    if message.content.strip().lower() == '/clear':
        await clear_all_documents()
        return
    
    if message.content.strip().lower() == '/files':
        await show_uploaded_files()
        return
    
    # HANDLE FILE UPLOADS
    if message.elements:
        await process_files(message.elements)
        return
    
    # HANDLE REGULAR MESSAGES WITH STREAMING
    thinking_msg = cl.Message(content="")
    await thinking_msg.send()
    
    # Initialize agent state
    initial_state: AgentState = {
        "query": message.content,
        "retrieved_chunks": [],
        "reasoning_steps": [],
        "final_answer": None,
        "error": None
    }
    
    # Run retrieval and reasoning nodes
    state_after_retrieval = await retrieve_context_node(initial_state)
    state_after_reasoning = await reasoning_node(state_after_retrieval)
    
    # Prepare messages for streaming
    messages = [{"role": "system", "content": system_prompt_content}]
    
    if state_after_reasoning['retrieved_chunks']:
        context_text = "\n\n---DOCUMENT CONTEXT---\n" + "\n\n".join(state_after_reasoning['retrieved_chunks']) + "\n---END CONTEXT---\n"
        messages.append({"role": "system", "content": context_text})
    
    for user_msg, assistant_msg in conversation_history[-5:]:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": assistant_msg})
    
    messages.append({"role": "user", "content": message.content})
    
    # Stream the response with automatic context management
    stream = await generate_completion_stream(messages)
    
    if stream:
        full_response = ""
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                token = chunk.choices[0].delta.content
                full_response += token
                await thinking_msg.stream_token(token)
        
        await thinking_msg.update()
        conversation_history.append((message.content, full_response))
    else:
        thinking_msg.content = "‚ùå Error generating response"
        await thinking_msg.update()