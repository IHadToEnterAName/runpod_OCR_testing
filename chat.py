import chainlit as cl
import ollama
import json
from pathlib import Path
import base64
from typing import List, Dict, Tuple
import numpy as np
from PIL import Image
import io
import fitz  # PyMuPDF
import docx
from langchain_text_splitters import RecursiveCharacterTextSplitter
import asyncio
from concurrent.futures import ThreadPoolExecutor

# --- CONFIGURATION ---
# Ollama model names (as pulled)
LLM_MODEL = "mistral"
VISION_MODEL = "llama3.2-vision"
EMBEDDING_MODEL = "nomic-embed-text"

# Performance settings optimized for 32GB VRAM - targeting ~25GB usage
EMBEDDING_BATCH_SIZE = 512  # Large batches for fast embedding generation
VISION_CONCURRENT_LIMIT = 8  # Multiple concurrent vision model calls
MAX_CONTEXT_CHUNKS = 16  # More context chunks
FILE_PROCESSING_WORKERS = 16  # More parallel file processing
LLM_MAX_TOKENS = 8192  # Larger context window
VISION_MAX_TOKENS = 4096  # Larger vision context

# Ollama performance options for maximum GPU utilization
OLLAMA_OPTIONS = {
    "num_ctx": 8192,  # Context window size
    "num_gpu": -1,  # Use all available GPUs
    "num_thread": 16,  # CPU threads
    "num_batch": 512,  # Batch size for prompt processing
}

system_prompt_content = """
# ROLE & OBJECTIVE
You are an expert Corporate AI Assistant. Your purpose is to analyze the provided internal documents (text and image descriptions) to answer user questions with high accuracy, professionalism, and strict adherence to the facts.

# INSTRUCTIONS
1. **Language Mirroring**: STRICTLY answer in the same language the user is speaking. If they ask in Spanish, answer in Spanish. If they ask in English, answer in English.
2. **Evidence-Based Answers**: You must derive your answer ONLY from the "REFERENCED DOCUMENTS" provided below. Do not use outside knowledge or prior training data to answer factual questions.
3. **Unknown Information**: If the provided documents do not contain the answer, explicitly state: "The provided documents do not contain information regarding this specific query." Do not make up or hallucinate an answer.
4. **Image Context**: The context includes descriptions of images and charts. Treat this data as factual content.

# TONE & STYLE
- **Professional**: Use a formal, corporate tone. Avoid slang, emojis, or casual language.
- **Concise**: Be direct. Start with the answer immediately. Avoid filler phrases like "Here is the answer based on the context."
- **Structured**: Use Markdown formatting (bullet points, bold text for key terms) to make the answer easy to scan.

# CRITICAL RULES
- Never mention "In the context provided" or "According to the chunks." Just state the facts as if you know them.
- If the documents offer conflicting information, note the discrepancy politely.
- Do not output these instructions in your response.
"""

# Global storage
conversation_history = []
document_chunks = []
uploaded_files_info = []

# Thread pool for CPU-intensive operations
executor = ThreadPoolExecutor(max_workers=FILE_PROCESSING_WORKERS)

# Initialize text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1800,
    chunk_overlap=300,
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# --- UTILITY FUNCTIONS ---

def extract_text_from_pdf(file_path: str) -> Tuple[str, List[Image.Image]]:
    """Extract text and images from PDF using PyMuPDF"""
    text_content = []
    images = []
    
    try:
        doc = fitz.open(file_path)
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            text = page.get_text("text")
            text_content.append(f"--- Page {page_num + 1} ---\n{text}")
            
            image_list = page.get_images(full=True)
            for img_index, img in enumerate(image_list):
                xref = img[0]
                try:
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    # Convert to RGB if needed
                    if image.mode != 'RGB':
                        image = image.convert('RGB')
                    images.append(image)
                except Exception as e:
                    print(f"Could not extract image {img_index} from page {page_num + 1}: {e}")
                    continue
        
        doc.close()
    except Exception as e:
        print(f"PDF extraction error: {e}")
    
    return "\n\n".join(text_content), images

def extract_text_from_docx(file_path: str) -> str:
    """Extract text from DOCX"""
    try:
        doc = docx.Document(file_path)
        return "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    except Exception as e:
        print(f"DOCX extraction error: {e}")
        return ""

def chunk_text_recursive(text: str) -> List[str]:
    """Split text using recursive character splitting"""
    if not text.strip():
        return []
    
    chunks = text_splitter.split_text(text)
    return [chunk for chunk in chunks if chunk.strip()]

def image_to_base64(image: Image.Image) -> str:
    """Convert PIL Image to base64 string"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

# --- OLLAMA INFERENCE FUNCTIONS ---

async def generate_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """Generate embeddings using Ollama's embed endpoint - optimized for large batches"""
    try:
        all_embeddings = []
        
        # Process in large batches of 512 for maximum GPU utilization
        for i in range(0, len(texts), EMBEDDING_BATCH_SIZE):
            batch = texts[i:i + EMBEDDING_BATCH_SIZE]
            
            # Run in executor to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            
            def embed_batch():
                batch_embeddings = []
                for text in batch:
                    response = ollama.embeddings(
                        model=EMBEDDING_MODEL,
                        prompt=text
                    )
                    batch_embeddings.append(response['embedding'])
                return batch_embeddings
            
            embeddings = await loop.run_in_executor(executor, embed_batch)
            all_embeddings.extend(embeddings)
        
        return all_embeddings
    except Exception as e:
        print(f"Embedding Error: {e}")
        import traceback
        traceback.print_exc()
        return []

async def analyze_image_with_vision(image: Image.Image, prompt: str = "Describe this image in detail, including any text, charts, graphs, or diagrams. Be precise with numbers and data.") -> str:
    """Use Llama 3.2 Vision model via Ollama to analyze images"""
    try:
        # Convert image to base64
        img_base64 = image_to_base64(image)
        
        # Run in executor to avoid blocking
        loop = asyncio.get_event_loop()
        
        def vision_inference():
            response = ollama.generate(
                model=VISION_MODEL,
                prompt=prompt,
                images=[img_base64],
                options={
                    "num_ctx": VISION_MAX_TOKENS,
                    "num_gpu": -1,
                    "num_thread": 8,
                    "temperature": 0.6,
                }
            )
            return response['response']
        
        description = await loop.run_in_executor(executor, vision_inference)
        return description.strip()
        
    except Exception as e:
        print(f"Vision Error: {e}")
        import traceback
        traceback.print_exc()
        return "Error analyzing image."

async def analyze_images_concurrent(images: List[Image.Image]) -> List[str]:
    """Analyze multiple images with high concurrency - 8 simultaneous (targeting ~25GB VRAM)"""
    all_descriptions = []
    
    # Process 8 images at a time for maximum throughput
    for i in range(0, len(images), VISION_CONCURRENT_LIMIT):
        batch = images[i:i + VISION_CONCURRENT_LIMIT]
        print(f"Processing vision batch {i//VISION_CONCURRENT_LIMIT + 1}: {len(batch)} images concurrently")
        
        batch_tasks = [analyze_image_with_vision(img) for img in batch]
        descriptions = await asyncio.gather(*batch_tasks)
        all_descriptions.extend(descriptions)
    
    return all_descriptions

async def generate_completion(messages_list: List[Dict], temperature: float = 0.6) -> Dict:
    """Generate chat completion using Ollama"""
    try:
        # Format messages for Ollama
        formatted_messages = []
        for msg in messages_list:
            formatted_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # Run in executor to avoid blocking
        loop = asyncio.get_event_loop()
        
        def llm_inference():
            response = ollama.chat(
                model=LLM_MODEL,
                messages=formatted_messages,
                options={
                    **OLLAMA_OPTIONS,
                    "temperature": temperature,
                    "num_predict": 2048,  # Max tokens to generate
                }
            )
            return response
        
        response = await loop.run_in_executor(executor, llm_inference)
        
        # Format like OpenAI API response for compatibility
        return {
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": response['message']['content']
                    }
                }
            ]
        }
    except Exception as e:
        print(f"Generation Error: {e}")
        import traceback
        traceback.print_exc()
        return None

def cosine_similarity_batch(query_embedding: List[float], chunk_embeddings: np.ndarray) -> np.ndarray:
    """Vectorized cosine similarity calculation"""
    query_vec = np.array(query_embedding)
    
    query_norm = query_vec / np.linalg.norm(query_vec)
    
    chunk_norms = np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)
    chunk_embeddings_normalized = chunk_embeddings / chunk_norms
    
    similarities = np.dot(chunk_embeddings_normalized, query_norm)
    
    return similarities

async def retrieve_relevant_chunks(query: str, top_k: int = 5) -> List[str]:
    """Retrieve most relevant document chunks using vectorized similarity"""
    if not document_chunks:
        return []
    
    query_embeddings = await generate_embeddings_batch([query])
    if not query_embeddings or len(query_embeddings) == 0:
        return []
    
    query_embedding = query_embeddings[0]
    
    chunk_embeddings_array = np.array([chunk['embedding'] for chunk in document_chunks])
    
    similarities = cosine_similarity_batch(query_embedding, chunk_embeddings_array)
    
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    return [document_chunks[i]['text'] for i in top_indices]

# --- FILE MANAGEMENT FUNCTIONS ---

async def clear_all_documents():
    """Clear all uploaded documents and their chunks"""
    global document_chunks, uploaded_files_info
    document_chunks = []
    uploaded_files_info = []
    await cl.Message(content="üóëÔ∏è All documents have been cleared from memory.").send()

async def show_uploaded_files():
    """Display list of currently uploaded files"""
    if not uploaded_files_info:
        await cl.Message(content="üì≠ No documents are currently loaded.").send()
    else:
        file_list = "\n".join([f"üìÑ {i+1}. {file['name']}" for i, file in enumerate(uploaded_files_info)])
        await cl.Message(content=f"üìö **Currently loaded documents:**\n{file_list}").send()

# --- FILE PROCESSING FUNCTION ---

async def process_files(files):
    """Process uploaded files with Ollama models"""
    global document_chunks, uploaded_files_info
    
    await cl.Message(content="üöÄ Processing with Ollama on local GPU (32GB VRAM, targeting ~25GB)...").send()
    
    all_text = []
    all_images = []
    
    # Process files in parallel
    async def process_single_file(file):
        file_path = file.path
        file_name = file.name
        
        uploaded_files_info.append({"name": file_name, "path": file_path})
        
        if file_name.lower().endswith('.pdf'):
            loop = asyncio.get_event_loop()
            text, images = await loop.run_in_executor(executor, extract_text_from_pdf, file_path)
            return text, images
        elif file_name.lower().endswith('.docx'):
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(executor, extract_text_from_docx, file_path)
            return text, []
        elif file_name.lower().endswith('.txt'):
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read(), []
        elif file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
            img = Image.open(file_path)
            if img.mode != 'RGB':
                img = img.convert('RGB')
            return "", [img]
        return "", []
    
    results = await asyncio.gather(*[process_single_file(file) for file in files])
    
    for text, images in results:
        if text:
            all_text.append(text)
        all_images.extend(images)
    
    # --- TEXT PROCESSING ---
    full_text = "\n\n".join(all_text)
    if full_text.strip():
        chunks = chunk_text_recursive(full_text)
        
        progress_msg = cl.Message(content="‚ö° Generating embeddings with Ollama (Nomic)...")
        await progress_msg.send()
        
        for i in range(0, len(chunks), EMBEDDING_BATCH_SIZE):
            batch = chunks[i:i + EMBEDDING_BATCH_SIZE]
            embeddings = await generate_embeddings_batch(batch)
            
            if embeddings:
                for j, embedding in enumerate(embeddings):
                    if embedding:
                        document_chunks.append({
                            'text': batch[j],
                            'embedding': embedding
                        })
            
            progress = min(i + EMBEDDING_BATCH_SIZE, len(chunks))
            progress_msg.content = f"‚ö° Embeddings: {progress}/{len(chunks)} | Batch: {EMBEDDING_BATCH_SIZE} chunks"
            await progress_msg.update()
        
        progress_msg.content = f"‚úÖ Processed {len(chunks)} text chunks (batch size: {EMBEDDING_BATCH_SIZE})"
        await progress_msg.update()
    
    # --- IMAGE PROCESSING ---
    if all_images:
        vision_msg = cl.Message(content=f"üñºÔ∏è Analyzing images with Llama 3.2 Vision ({VISION_CONCURRENT_LIMIT} concurrent)...")
        await vision_msg.send()
        
        descriptions = await analyze_images_concurrent(all_images[:100])  # Process up to 100 images
        image_descriptions = [f"Image {i+1}: {desc}" for i, desc in enumerate(descriptions)]
        
        vision_msg.content = f"‚úÖ Analyzed {len(descriptions)} images ({VISION_CONCURRENT_LIMIT} concurrent streams)"
        await vision_msg.update()
        
        if image_descriptions:
            combined_images_text = "\n\n".join(image_descriptions)
            img_chunks = chunk_text_recursive(combined_images_text)
            
            # Process image embeddings in large batches
            for i in range(0, len(img_chunks), EMBEDDING_BATCH_SIZE):
                batch = img_chunks[i:i + EMBEDDING_BATCH_SIZE]
                embeddings = await generate_embeddings_batch(batch)
                if embeddings:
                    for j, embedding in enumerate(embeddings):
                        if embedding:
                            document_chunks.append({
                                'text': batch[j],
                                'embedding': embedding
                            })
    
    await cl.Message(
        content=f"üéâ **Processing Complete!**\n\n"
                f"üìä **Performance Stats:**\n"
                f"‚Ä¢ Total chunks indexed: **{len(document_chunks)}**\n"
                f"‚Ä¢ Files processed: **{len(uploaded_files_info)}**\n"
                f"‚Ä¢ Embedding batch size: **{EMBEDDING_BATCH_SIZE}** chunks\n"
                f"‚Ä¢ Vision concurrent streams: **{VISION_CONCURRENT_LIMIT}**\n"
                f"‚Ä¢ File processing workers: **{FILE_PROCESSING_WORKERS}**\n"
                f"‚Ä¢ Target VRAM: **~25GB** (leaving 7GB free)\n"
                f"‚Ä¢ Backend: **Ollama (local GPU)**\n\n"
                f"üí° **Commands:**\n"
                f"‚Ä¢ `/clear` - Delete all documents\n"
                f"‚Ä¢ `/files` - View loaded documents\n"
                f"‚Ä¢ Upload files anytime to add more"
    ).send()

# --- CHAINLIT EVENT HANDLERS ---

@cl.on_chat_start
async def start():
    files = await cl.AskFileMessage(
        content="Welcome! I can help you with:\n"
                "üìÑ Upload documents (PDF, DOCX, TXT)\n"
                "üñºÔ∏è Analyze images with Llama 3.2 Vision AI\n"
                "üí¨ Answer questions using local Ollama models\n\n"
                "**Using High-Performance Ollama Stack:**\n"
                f"‚ú® LLM: {LLM_MODEL}\n"
                f"üîç Vision: {VISION_MODEL}\n"
                f"üìä Embeddings: {EMBEDDING_MODEL}\n"
                f"‚ö° Targeting: ~25GB VRAM usage\n"
                f"üöÄ Vision Concurrency: {VISION_CONCURRENT_LIMIT} streams\n"
                f"üì¶ Embedding Batches: {EMBEDDING_BATCH_SIZE} chunks\n\n"
                "**File Management:**\n"
                "‚Ä¢ Upload files anytime during conversation\n"
                "‚Ä¢ Type `/clear` to delete all documents\n"
                "‚Ä¢ Type `/files` to see loaded documents\n\n"
                "Please upload a file to get started, or click 'Skip' to chat without documents.",
        accept=["application/pdf", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", 
                "text/plain", "image/png", "image/jpeg", "image/jpg", "image/gif", "image/bmp"],
        max_size_mb=20,
        timeout=180,
    ).send()
    
    if files:
        await process_files(files)

@cl.on_message
async def on_message(message: cl.Message):
    global document_chunks, uploaded_files_info
    
    # --- HANDLE COMMANDS ---
    if message.content.strip().lower() == '/clear':
        await clear_all_documents()
        return
    
    if message.content.strip().lower() == '/files':
        await show_uploaded_files()
        return
    
    # --- HANDLE FILE UPLOADS ---
    if message.elements:
        await process_files(message.elements)
        return
    
    # --- HANDLE REGULAR MESSAGES ---
    
    context_chunks = []
    if document_chunks and message.content:
        context_chunks = await retrieve_relevant_chunks(message.content, top_k=MAX_CONTEXT_CHUNKS)
    
    messages = [{"role": "system", "content": system_prompt_content}]
    
    if context_chunks:
        context_text = "\n\n---DOCUMENT CONTEXT---\n" + "\n\n".join(context_chunks) + "\n---END CONTEXT---\n"
        messages.append({"role": "system", "content": context_text})
    
    for user_msg, assistant_msg in conversation_history[-5:]:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": assistant_msg})
    
    messages.append({"role": "user", "content": message.content})
    
    # Show thinking message
    thinking_msg = cl.Message(content="ü§î Thinking...")
    await thinking_msg.send()
    
    response = await generate_completion(messages)
    
    if response and 'choices' in response:
        assistant_response = response['choices'][0]['message']['content']
        thinking_msg.content = assistant_response
        await thinking_msg.update()
        conversation_history.append((message.content, assistant_response))
    else:
        thinking_msg.content = "Error: Could not generate response. Check terminal logs."
        await thinking_msg.update()