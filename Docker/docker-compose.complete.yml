version: '3.8'

# =============================================================================
# Complete RAG System - All-in-Docker
# Everything runs in containers for portability
# Optimized for RTX 5090
# =============================================================================

services:
  # ===========================================================================
  # REDIS - Caching (Future Use)
  # ===========================================================================
  redis:
    image: redis/redis-stack:7.2.0-v6
    container_name: rag_redis
    ports:
      - "6379:6379"
      - "8002:8001"  # RedisInsight UI
    volumes:
      - redis_data:/data
    environment:
      - REDIS_ARGS=--save 60 1 --loglevel warning
    command: redis-stack-server
    restart: unless-stopped
    networks:
      - rag_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # ===========================================================================
  # CHROMADB - Vector Database
  # ===========================================================================
  chromadb:
    image: chromadb/chroma:0.4.22
    container_name: rag_chromadb
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    restart: unless-stopped
    networks:
      - rag_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # vLLM VISION SERVER (Qwen2.5-VL-3B)
  # Port 8006, GPU 0, 0.3 memory utilization
  # ===========================================================================
  vllm_vision:
    build:
      context: .
      dockerfile: Dockerfile.complete
    container_name: rag_vllm_vision
    ports:
      - "8006:8006"
    volumes:
      - huggingface_cache:/workspace/huggingface
      - vllm_cache:/workspace/vllm_cache
      - models_cache:/workspace/models
    environment:
      - HF_HOME=/workspace/huggingface
      - VLLM_CACHE_ROOT=/workspace/vllm_cache
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_USE_V1=0
    command: >
      vllm serve "Qwen/Qwen2.5-VL-3B-Instruct"
      --port 8006
      --host 0.0.0.0
      --gpu-memory-utilization 0.3
      --max-model-len 8192
      --limit-mm-per-prompt '{"image":12}'
      --enforce-eager
      --trust-remote-code
    restart: unless-stopped
    networks:
      - rag_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # ===========================================================================
  # vLLM REASONING SERVER (DeepSeek-R1-7B)
  # Port 8005, GPU 0, 0.53 memory utilization
  # ===========================================================================
  vllm_reasoning:
    build:
      context: .
      dockerfile: Dockerfile.complete
    container_name: rag_vllm_reasoning
    ports:
      - "8005:8005"
    volumes:
      - huggingface_cache:/workspace/huggingface
      - vllm_cache:/workspace/vllm_cache
      - models_cache:/workspace/models
    environment:
      - HF_HOME=/workspace/huggingface
      - VLLM_CACHE_ROOT=/workspace/vllm_cache
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_USE_V1=0
    command: >
      vllm serve "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
      --port 8005
      --host 0.0.0.0
      --gpu-memory-utilization 0.53
      --max-model-len 16384
      --enforce-eager
      --enable-prefix-caching
    restart: unless-stopped
    networks:
      - rag_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # ===========================================================================
  # RAG APPLICATION (Chainlit + RAG Pipeline)
  # ===========================================================================
  rag_app:
    build:
      context: .
      dockerfile: Dockerfile.complete
    container_name: rag_app
    depends_on:
      chromadb:
        condition: service_healthy
      vllm_vision:
        condition: service_healthy
      vllm_reasoning:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - ./src:/workspace/src
      - ./data:/workspace/data
      - huggingface_cache:/workspace/huggingface
      - chroma_data:/workspace/chroma_db
    environment:
      # Service connections (internal Docker network)
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      
      # vLLM endpoints (internal Docker network)
      - VISION_URL=http://vllm_vision:8006/v1
      - REASONING_URL=http://vllm_reasoning:8005/v1
      
      # Model configuration
      - VISION_MODEL=Qwen/Qwen2.5-VL-3B-Instruct
      - REASONING_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
      - EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      
      # Paths
      - PYTHONPATH=/workspace/src
      - HF_HOME=/workspace/huggingface
      - CUDA_VISIBLE_DEVICES=0
    command: python -m chainlit run /workspace/src/app.py --host 0.0.0.0 --port 8000
    restart: unless-stopped
    networks:
      - rag_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
        limits:
          memory: 8G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

# =============================================================================
# VOLUMES - Persistent Storage
# =============================================================================
volumes:
  redis_data:
    driver: local
  chroma_data:
    driver: local
  huggingface_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/huggingface
  vllm_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/vllm_cache
  models_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/models

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  rag_network:
    driver: bridge

# =============================================================================
# USAGE INSTRUCTIONS
# =============================================================================
#
# 1. Build images:
#    docker compose build
#
# 2. Start all services:
#    docker compose up -d
#
# 3. View logs:
#    docker compose logs -f
#
# 4. Check status:
#    docker compose ps
#
# 5. Access services:
#    - RAG App: http://localhost:8000
#    - ChromaDB: http://localhost:8001
#    - RedisInsight: http://localhost:8002
#    - Vision API: http://localhost:8006/v1/models
#    - Reasoning API: http://localhost:8005/v1/models
#
# 6. Stop services:
#    docker compose down
#
# 7. Full cleanup (removes volumes):
#    docker compose down -v
#
# =============================================================================
# GPU ALLOCATION
# =============================================================================
#
# Single GPU Setup (GPU 0):
# - Vision Model: 0.3 GPU memory utilization
# - Reasoning Model: 0.53 GPU memory utilization
# - RAG App: embeddings
#
# =============================================================================
